{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Implementation of an artificial neural network\n",
    "# Load all necessary libraries\n",
    "from __future__ import division, print_function\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from operator import add\n",
    "from datetime import datetime \n",
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns a dictionary of all kmers per reads and a list of all unique kmers in dataset\n",
    "def get_kmers(k_value, dict_seq):\n",
    "    unique_kmers = set()\n",
    "    reads_list_kmer = {} # key = read id, value = list of all consecutive kmers in reads (#kmers in read = 150 - k + 1)\n",
    "    list_unusual_reads = []\n",
    "    for read, seq in dict_seq.items():  \n",
    "        reads_list_kmer[read] = []\n",
    "        for n in range(len(seq)):\n",
    "            if seq[n] == 'N':\n",
    "#                 print('Read qui fait chier: {}'.format(read))\n",
    "                if read not in list_unusual_reads:\n",
    "                    list_unusual_reads.append(read)\n",
    "            if n < len(seq) - (k_value - 1):\n",
    "                kmer = str(seq[n:n+k_value])\n",
    "                reads_list_kmer[read].append(kmer)\n",
    "                if kmer not in unique_kmers:\n",
    "                    unique_kmers.add(kmer)\n",
    "    # remove all unusual reads from reads_list_kmer\n",
    "    print(len(list_unusual_reads))\n",
    "    print(len(reads_list_kmer))\n",
    "    for x in range(len(list_unusual_reads)):\n",
    "        del reads_list_kmer[list_unusual_reads[x]]\n",
    "    print(len(reads_list_kmer))\n",
    "            \n",
    "    \n",
    "    return reads_list_kmer, unique_kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(reads_df, sequence_length, reads_list_kmer, kmers_mapping):\n",
    "    # Create matrix of zeros, where each row corresponds to a sequence of size of sequence_length\n",
    "    sequences = np.zeros((reads_df.shape[0], sequence_length), dtype=int)\n",
    "    # Fill the index of words in each sequence from the right-hand side of the matrix\n",
    "    for i in range(reads_df.shape[0]):\n",
    "    #     print(reads_order_df.loc[i,'read_id'])\n",
    "        # Get list of k-mers for read on row i of dataframe\n",
    "        list_kmers = reads_list_kmer[reads_df.loc[i,'read_id']]  \n",
    "        # convert list of kmers into a list of their mapped integers\n",
    "        list_int = [kmers_mapping.get(key) for key in list_kmers]\n",
    "    #     print(list_int)\n",
    "        # transform list into numpy array\n",
    "        array_int = np.asarray(list_int)\n",
    "        # Get the first n (sequence_length) kmers and flip array (?)\n",
    "    #     print(array_int)\n",
    "        array_int = np.flip(array_int[:sequence_length], axis=0)\n",
    "        # Replace array in corresponding position in sequences matrix\n",
    "        sequences[i] = array_int\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(reads_df):\n",
    "    # For each order get indices of reads in dataframe reads_order_df\n",
    "    list_orders = reads_df.order.unique().tolist()\n",
    "    # Enumerate the class labels starting at 0\n",
    "    class_mapping = {label:idx for idx, label in enumerate(list_orders)}\n",
    "    print(class_mapping, file=sys.stderr)\n",
    "    train_idx = []\n",
    "#     test_idx = []\n",
    "    for i in range(len(list_orders)):\n",
    "        # Get list of indices in dataframe of order\n",
    "        list_indices = reads_df.index[reads_df['order'] == list_orders[i]].tolist()\n",
    "        # Convert name to integer value given in class_mapping dictionary\n",
    "        reads_df.loc[list_indices,'order'] = class_mapping[list_orders[i]]\n",
    "        # shuffle list of indices\n",
    "#         random.shuffle(list_indices)\n",
    "        # select 70% of indices for the training set \n",
    "#         num_genomes_train = int(0.7*len(list_indices))\n",
    "        num_genomes_train = int(1*len(list_indices))\n",
    "        train_idx += list_indices[:num_genomes_train]\n",
    "        # select the remaining 30% of indices for the test set \n",
    "#         test_idx += list_indices[num_genomes_train:]\n",
    "    \n",
    "    return train_idx, class_mapping\n",
    "#     return train_idx, test_idx, class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate batches of data\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    # get the number of batches of size 64 given the size of the training dataset\n",
    "    n_batches = len(x)//batch_size # 2 / will round the float number to an integer\n",
    "    # create empty array to store y values in case it is not none\n",
    "    y_copy = np.empty([1, n_batches*batch_size])\n",
    "    if y is not None:\n",
    "        y_copy = np.array(y)\n",
    "        y_copy = y_copy[:n_batches*batch_size]\n",
    "    # create batch from reduced training set \n",
    "    x_copy = np.array(x)\n",
    "    x_copy = x_copy[:n_batches*batch_size]\n",
    "    for i in range(0, x_copy.shape[0], batch_size):\n",
    "        if y is not None:\n",
    "            yield (x_copy[i:i+batch_size, :], y_copy[i:i+batch_size])\n",
    "        else:\n",
    "            yield x_copy[i:i+batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reads_seq(path):\n",
    "    reads_seq = {} # keys = read_id, value = read sequence\n",
    "    reads_sizes = {} # keys = size, value = # of reads with that size\n",
    "    file_datasets = path + 'list_datasets.txt'\n",
    "    with open(file_datasets, 'r') as list_datasets:\n",
    "        for dataset in list_datasets:\n",
    "            dataset = str(dataset.strip('\\n'))\n",
    "            fList = os.listdir(path + dataset)\n",
    "            with open(path + dataset +  '/' + 'list_genomes.txt','r') as list_genomes:\n",
    "                num_read = 0\n",
    "                for line in list_genomes:\n",
    "                    line = str(line.strip('\\n'))\n",
    "                    newline = line.split('\\t')\n",
    "                    order_name = str(newline[0])\n",
    "                    genome_id = str(newline[1])\n",
    "        #             order_genome[genome_id] = order_name\n",
    "                    # Open fastq file with reads simulated with genome fasta sequence\n",
    "                    file_fastq = path + dataset +  '/' + order_name + '_reads.fq'\n",
    "                    list_reads = []\n",
    "                    with open(file_fastq, 'r') as reads_f:\n",
    "                        num_line = 0\n",
    "                        read_id = str()\n",
    "                        for line in reads_f: \n",
    "                            line = str(line.strip('\\n'))\n",
    "                            if num_line == 1:\n",
    "                                seq = line\n",
    "                                reads_seq[read_id] = seq\n",
    "                                num_read += 1\n",
    "                                num_line = 0\n",
    "                                read_id = str()\n",
    "                            if line[0:4] == '@S0R':\n",
    "                                read_id = line + '_' + genome_id\n",
    "                                list_reads.append(line[4])\n",
    "                                num_line = 1\n",
    "\n",
    "    return reads_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reads_info(path, dict_reads_updated):\n",
    "    reads_order_dict = {} # keys = read id and values - name of order\n",
    "    order_genome = {} # keys = genome_id, value = order\n",
    "    file_datasets = path + 'list_datasets.txt'\n",
    "    with open(file_datasets, 'r') as list_datasets:\n",
    "        num_read = 0\n",
    "        for dataset in list_datasets:\n",
    "            dataset = str(dataset.strip('\\n'))\n",
    "            fList = os.listdir(path + dataset)\n",
    "            with open(path + dataset +  '/' + 'list_genomes.txt','r') as list_genomes:\n",
    "                for line in list_genomes:\n",
    "                    line = str(line.strip('\\n'))\n",
    "                    newline = line.split('\\t')\n",
    "                    order_name = str(newline[0])\n",
    "                    genome_id = str(newline[1])\n",
    "                    order_genome[genome_id] = order_name\n",
    "                    # Open fastq file with reads simulated with genome fasta sequence\n",
    "                    file_fastq = path + dataset +  '/' + order_name + '_reads.fq'\n",
    "                    list_reads = []\n",
    "                    with open(file_fastq, 'r') as reads_f:\n",
    "                        read_id = str()\n",
    "                        for line in reads_f: \n",
    "                            line = str(line.strip('\\n'))\n",
    "                            if line[0:4] == '@S0R':\n",
    "                                read_id = line + '_' + genome_id\n",
    "                                if read_id in dict_reads_updated:\n",
    "                                    reads_order_dict[num_read] = [read_id, order_name]\n",
    "                                    num_read += 1\n",
    "\n",
    "    return reads_order_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(X_test, Y_test, rnn, class_mapping):\n",
    "    # Use trained model for predicting the class labels on the test set\n",
    "    preds = rnn.predict(X_test)\n",
    "    # Get the ground values of sequences labels (first n=number of labels)\n",
    "    Y_true = Y_test[:len(preds)]\n",
    "    print(' Test Accuracy: %.3f' % (np.sum(preds == Y_true) / len(Y_true)), file=sys.stderr)\n",
    "    # Create confusion matrix for each validation set\n",
    "    cm = pd.DataFrame(0, columns=list(np.unique(Y_true)), index=list(np.unique(Y_true)))\n",
    "    # Fill cm\n",
    "    for i in range(len(Y_true)):\n",
    "        cm.iloc[cm.index.get_loc(Y_true[i]),cm.columns.get_loc(preds[i])] += 1\n",
    "    print(cm, file=sys.stderr)\n",
    "    print(np.unique(Y_true), file=sys.stderr)\n",
    "    list_labels = np.unique(Y_true)\n",
    "    # compute recall, precision for different labels\n",
    "    for j in range(len(list_labels)):\n",
    "        # get name of labels\n",
    "        label = str()\n",
    "        for order, integer in class_mapping.items():\n",
    "            if integer == list_labels[j]:\n",
    "                label = order\n",
    "        true_positives = cm.iloc[list_labels[j],list_labels[j]]\n",
    "        false_positives = cm.iloc[:,list_labels[j]].sum() - true_positives\n",
    "        false_negatives = cm.iloc[list_labels[j],:].sum() - true_positives\n",
    "        recall = true_positives / (false_negatives + true_positives)\n",
    "        precision = true_positives / (false_positives + true_positives)\n",
    "        # Print out Recall and Precision for said order\n",
    "        print('Recall %s: %.2f%%' % (label, 100*recall), file=sys.stderr)\n",
    "        print('Precision %s: %.2f%%' % (label, 100*precision), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_kmers(list_nt, prefix, n, k, list_kmers):\n",
    "    if k == 0 :\n",
    "        list_kmers.append(prefix)\n",
    "        return list_kmers\n",
    "    \n",
    "    for i in range(n):\n",
    "        newPrefix = prefix + list_nt[i]\n",
    "        get_all_kmers(list_nt, newPrefix, n, k-1, list_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Model parameters\n",
    "    embedding_size = 6\n",
    "    num_layers = 1\n",
    "    lstm_size = 256\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 30\n",
    "    batch_size = 64\n",
    "    k_value = 10\n",
    "    num_kmers = 4**k_value\n",
    "    num_classes = 8\n",
    "    list_kmers = []\n",
    "    list_nt = ['A', 'T', 'C', 'G']\n",
    "    get_all_kmers(list_nt, \"\", len(list_nt), k_value, list_kmers)\n",
    "#     print('List of kmers: {}'.format(list_kmers), file=sys.stderr)\n",
    "    print('Number of {}-mers in list_kmers: {}'.format(k_value ,len(list_kmers)))\n",
    "    # For each kmer assign an integer randomly between 0 and number of unique kmers in dataset\n",
    "    # generate a list of integers\n",
    "    list_num = list(range(0,num_kmers))\n",
    "    # Assign integer to each unique kmers\n",
    "    kmers_mapping = dict(zip(list_kmers, list_num))\n",
    "#     reads_order_dict, reads_seq = get_reads('/glade/u/home/ccres/data/fastq_files_2/Reads_7_orders_1X/')\n",
    "#     reads_order_dict, reads_seq = get_reads('/Users/Cissou/Desktop/fastq_files_3/Reads_8_orders_1X_1/')\n",
    "#     reads_order_dict, reads_seq = get_reads('/users/ccres/data/ccres/rnn_meta_classifier/fastq_files_6/Reads_7_orders_7X/')\n",
    "    # Get sequences for each read\n",
    "#     reads_seq = get_reads_seq('/Users/Cissou/Desktop/fastq_files_3/Data/')\n",
    "    reads_seq = get_reads_seq('/users/ccres/data/ccres/rnn_meta_classifier/fastq_files_7/Data/')\n",
    "    print('Number of reads: {}'.format(len(reads_seq)))\n",
    "    print('k value: {0}'.format(k_value))\n",
    "#     # Get kmers from removes all reads with unusual characters\n",
    "#     # Get kmers\n",
    "    reads_list_kmer, unique_kmers = get_kmers(k_value, reads_seq)\n",
    "# #     print(unique_kmers)\n",
    "#     # Print out total number of kmers in dataset \n",
    "#     n_kmers = len(unique_kmers)\n",
    "#     print('Number of {0}-mers in dataset: {1}'.format(k,n_kmers), file=sys.stderr)\n",
    "#     print('Number of {0}-mers: {1}'.format(k,num_kmers), file=sys.stderr)\n",
    "#     # get info on reads dataset updated\n",
    "#     reads_order_dict = get_reads_info('/Users/Cissou/Desktop/fastq_files_3/Reads_8_orders_1X_1/',reads_list_kmer)\n",
    "    reads_order_dict = get_reads_info('/users/ccres/data/ccres/rnn_meta_classifier/fastq_files_7/Data/',reads_list_kmer)\n",
    "    print('Number of reads: {}'.format(len(reads_order_dict)))\n",
    "#     # Create dataframe of reads\n",
    "#     # Create list of names for columns in dataframe\n",
    "    headers = ['read_id'] + ['order']\n",
    "    # Create dataframe\n",
    "    reads_order_df = pd.DataFrame.from_dict(reads_order_dict, orient='index')\n",
    "    reads_order_df.columns = headers\n",
    "    list_orders = reads_order_df.order.unique().tolist()\n",
    "    class_mapping = {label:idx for idx, label in enumerate(list_orders)}\n",
    "    print(class_mapping)\n",
    "    print(reads_order_df.head())\n",
    "    print(reads_order_df.tail())\n",
    "    print(reads_order_df.order.unique())\n",
    "    print(reads_order_df.shape)\n",
    "    \n",
    "    print(\"\\nProcess reads: START -- {}\".format(datetime.now()))\n",
    "    # Set size of vector representing each read\n",
    "    sequence_length = int(150 - k_value + 1)\n",
    "    print('Vector size with k = {0}: {1}'.format(k_value, sequence_length))\n",
    "    # Create matrix of vectors\n",
    "    sequences = create_matrix(reads_order_df, sequence_length, reads_list_kmer, kmers_mapping)\n",
    "    print(\"\\nProcess reads: END -- {}\".format(datetime.now()))\n",
    "    # Convert order names to integers in dataframe\n",
    "    print('List of orders in training set: {}'.format(reads_order_df.order.unique()))\n",
    "    for n in range(len(list_orders)):\n",
    "        # Get list of indices in dataframe of order\n",
    "        print(list_orders[n])\n",
    "        list_indices = reads_order_df.index[reads_order_df['order'] == list_orders[n]].tolist()\n",
    "        print(len(list_indices))\n",
    "        # Convert name to integer value given in class_mapping dictionary\n",
    "        reads_order_df.loc[list_indices,'order'] = class_mapping[list_orders[n]]\n",
    "    print('List of orders in training set: {}'.format(reads_order_df.order.unique()))\n",
    "#     # Get entire dataset into training set\n",
    "    train_idx = reads_order_df.index.values.tolist()\n",
    "    print('Size of training set: {}'.format(len(train_idx)))\n",
    "    random.shuffle(train_idx)\n",
    "    X_train = sequences[train_idx,:]\n",
    "    Y_train = reads_order_df.loc[train_idx,'order'].values\n",
    "    print('Size of vector X train: {}'.format(len(X_train)))\n",
    "    print('Size of vector Y train: {}'.format(len(Y_train)))\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Create new empty computation graph\n",
    "    g = tf.Graph()\n",
    "    # Define model\n",
    "    with g.as_default():\n",
    "        # Set a graph-level seed to make the random sequences generated by all operations be repeatable across sessions\n",
    "        tf.set_random_seed(123)\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            # Define the placeholders for holding the input values: sequences of unique kmers (tf_x) \n",
    "            # and the response values (tf_y)\n",
    "            tf_x = tf.placeholder(dtype=tf.int32, shape=[batch_size, sequence_length], name='tf_x')\n",
    "            print(tf_x)\n",
    "            tf_y = tf.placeholder(dtype=tf.int32, shape=[batch_size], name='tf_y')\n",
    "            print(tf_y)\n",
    "            tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "            print(tf_keepprob)\n",
    "            # Convert labels to a one-hot representation (necessary to use the cost function)\n",
    "            y_onehot = tf.one_hot(indices=tf_y, depth=num_classes)\n",
    "            print(y_onehot, file=sys.stderr)\n",
    "            # Create embedding layer: Create a matrix of size [n_kmers x embedding_size] as a tensor variable and initialize its elements\n",
    "            # randomly with floats between [-1,1]\n",
    "            print(\"\\nCreate embedding layer: START\")\n",
    "            print(datetime.now())\n",
    "            embedding = tf.Variable(tf.random_uniform((num_kmers, embedding_size), minval=-1, maxval=1), name='embedding')\n",
    "            print(embedding)\n",
    "            embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
    "            print(embed_x)\n",
    "            print(\"\\nCreate embedding layer + embed_x: DONE\")\n",
    "            print(datetime.now())\n",
    "            # define LSTM cell and stack them together\n",
    "            # BasicLSTMCell wrapper class that defines LSTM cells which can be stacked together to form a multilayer RNN\n",
    "            # using the MultiRNNCell wrapper class, apply dropout (NN regularization) to each layer\n",
    "            # make a list using python list comprehension of LSTM cells according to the desired number of RNN layers\n",
    "            cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                [tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.nn.rnn_cell.LSTMCell(lstm_size), \n",
    "                    output_keep_prob=tf_keepprob)\n",
    "                    for i in range(num_layers)])\n",
    "    #         tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell')\n",
    "            # Define the initial state (there are 3 types of inputs in LSTM cells: input data x (embed_x data tensor), \n",
    "            # activations of hidden units\n",
    "            # from the previous time step h and the cell state from the previous time step C)\n",
    "            # when we start processing a new input sequence, we initialize the cell states to zero state\n",
    "            initial_state = cells.zero_state(\n",
    "                batch_size, tf.float32)\n",
    "            print(' << initial state >>', initial_state)\n",
    "        with tf.device('/device:GPU:1'):\n",
    "            # the tf.nn.dynamic_rnn function pulls the embedded data, the RNN cells and their initial states and creates\n",
    "            # a pipeline for them according to the architecture of LSTM cells\n",
    "            # We store the final state to use  as the initial state of the next mini-batch of data\n",
    "    #         lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "    #             cells, embed_x, initial_state=self.inital_state)\n",
    "            lstm_outputs, final_state = tf.nn.dynamic_rnn(\n",
    "                cells, embed_x, initial_state=initial_state)\n",
    "            # the tf.nn.dynamic_rnn function returns a tuple containing the activations of the RNN cells: outputs and their\n",
    "            # final state: state. The output is a 3 dimensional tensor with the following shape:\n",
    "            # lstm_outputs shape: [batch_size, max_time, cells.output_size]\n",
    "            print('\\n << lstm_output >>', lstm_outputs)\n",
    "            print('\\n << final state >>', final_state)\n",
    "            print(lstm_outputs[:, -1])\n",
    "            # Pass outputs to a connected layer to get logits \n",
    "            logits = tf.layers.dense(inputs=lstm_outputs[:, -1], units=num_classes, activation=None, name='logits')\n",
    "            print('\\n << logits >>', logits)\n",
    "            y_proba = tf.nn.softmax(logits, name='probabilities')\n",
    "            predictions = {\n",
    "                'labels': tf.argmax(logits, axis=1, name='labels'),\n",
    "                'probabilities': y_proba\n",
    "            }\n",
    "\n",
    "            print('\\n << predictions >>', predictions)\n",
    "            # Define the cost function\n",
    "            cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y_onehot),\n",
    "                name='cost')\n",
    "            print('\\n << cost >>', cost)\n",
    "            # Define the optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "        # Create saver object\n",
    "        saver = tf.train.Saver()\n",
    "        # Returns an Operation that initializes global variables in the graph\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStart training: {}\".format(datetime.now()))\n",
    "    with tf.Session(graph=g) as sess:\n",
    "            sess.run(init_op)\n",
    "            iteration = 1\n",
    "            plot_cost = []\n",
    "            preds_all = []  # keep test accuracy after each epoch for testing set\n",
    "            # Train model 10 times with different sets of data \n",
    "            for epoch in range(num_epochs):\n",
    "                # start from the zero states of RNN cells as our current state\n",
    "                state = sess.run(initial_state)\n",
    "                training_costs = []\n",
    "                batch_generator = create_batch_generator(X_train, Y_train, batch_size=64)\n",
    "                for batch_x, batch_y in batch_generator:\n",
    "                    # feeding the current state along with the data batch_x and their labels batch_y\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                                'tf_y:0': batch_y,\n",
    "                                'tf_keepprob:0': 0.5,\n",
    "                                initial_state: state}\n",
    "                    # At the end of a mini-batch, update the state to be the final state\n",
    "                    batch_loss, _, state = sess.run(['cost:0', 'train_op', final_state], feed_dict=feed)\n",
    "                    training_costs.append(batch_loss)\n",
    "                    # Print train loss of model every 20 epochs\n",
    "#                     if iteration % 20 == 0:\n",
    "                    print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                                 \"| Train loss: %.5f\" %(epoch + 1, num_epochs, iteration, batch_loss))\n",
    "                    print(' -- Epoch %2d ''Avg. Training Loss: %.4f' % (epoch+1, np.mean(training_costs)))\n",
    "                    iteration += 1\n",
    "                # Get average training costs/test accuracy in batches for each epoch\n",
    "                plot_cost.append(np.mean(training_costs))  \n",
    "                # Save model after every epoch\n",
    "#                 if (epoch+1)%3 == 0:\n",
    "#                   self.saver.save(sess, \"model/SpeciesRNN-%d.ckpt\" % epoch)\n",
    "                saver.save(sess, \"model-8orders-sw-k{0}-cov1-100%-WO-Class/SpeciesRNN-sw-8orders-{0}-{1}.ckpt\".format(k_value,epoch))\n",
    "                # Test model with testing set\n",
    "                preds_batches = []\n",
    "                for ii, batch_x_1 in enumerate(create_batch_generator(X_train, None, batch_size=batch_size), 1):\n",
    "                    feed = {'tf_x:0':batch_x_1, 'tf_keepprob:0': 1.0, initial_state: state}\n",
    "                    pred, prob, test_state = sess.run(['labels:0', 'probabilities:0', state], feed_dict=feed)\n",
    "                    preds_batches.append(pred)\n",
    "                preds_all.append(np.mean(preds_batches))\n",
    "                \n",
    "            # Generate plot to visualize the training cost after each epoch\n",
    "            plt.clf()\n",
    "            plt.plot(range(1,len(plot_cost) + 1), plot_cost)\n",
    "            plt.ylabel('Training cost',fontsize=15)\n",
    "            plt.xlabel('Epoch',fontsize=15)\n",
    "            plt.savefig('/users/ccres/data/ccres/run/Training_Loss_{0}_k{1}_8orders_sw_RNN_cov1_100%-WO-Class.png'.format(learning_rate, k_value))\n",
    "            # Generate plot to visualize the training cost after each epoch\n",
    "            plt.clf()\n",
    "            plt.plot(range(1,len(preds_all) + 1), preds_all)\n",
    "            plt.ylabel('Test accuracy',fontsize=15)\n",
    "            plt.xlabel('Epoch',fontsize=15)\n",
    "            plt.savefig('/users/ccres/data/ccres/run/Test_Accuracy_TrainingSet_{0}_k{1}_8orders_sw_RNN_cov1_100%-WO-Class.png'.format(learning_rate, k_value))\n",
    "    print(\"\\nEnd training: {}\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of 10-mers in list_kmers: 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reads_8_orders_1X_1\n",
      "['Chlamydiales_reads.fq', 'Chlamydiales_reads.fq.gz', 'Enterobacterales_reads.fq', 'Enterobacterales_reads.fq.gz', 'Fusobacteriales_reads.fq', 'Fusobacteriales_reads.fq.gz', 'list_genomes.txt', 'Mycoplasmatales_reads.fq', 'Mycoplasmatales_reads.fq.gz', 'Other_reads.fq', 'Other_reads.fq.gz', 'Rhodobacterales_reads.fq', 'Rhodobacterales_reads.fq.gz', 'Spirochaetales_reads.fq', 'Spirochaetales_reads.fq.gz', 'Vibrionales_reads.fq', 'Vibrionales_reads.fq.gz']\n",
      "GCF_000005845.2\n",
      "GCF_000154745.2\n",
      "GCF_000027345.1\n",
      "GCF_000008685.2\n",
      "GCF_000008725.1\n",
      "GCF_000007325.1\n",
      "GCF_000006745.1\n",
      "GCF_000193595.2\n",
      "165926\n",
      "Reads_8_orders_1X_2\n",
      "['Chlamydiales_reads.fq', 'Chlamydiales_reads.fq.gz', 'Enterobacterales_reads.fq', 'Enterobacterales_reads.fq.gz', 'Fusobacteriales_reads.fq', 'Fusobacteriales_reads.fq.gz', 'list_genomes.txt', 'Mycoplasmatales_reads.fq', 'Mycoplasmatales_reads.fq.gz', 'Other_reads.fq', 'Other_reads.fq.gz', 'Rhodobacterales_reads.fq', 'Rhodobacterales_reads.fq.gz', 'Spirochaetales_reads.fq', 'Spirochaetales_reads.fq.gz', 'Vibrionales_read.fq.gz', 'Vibrionales_reads.fq']\n",
      "GCF_000007445.1\n",
      "GCF_000154765.2\n",
      "GCF_000143945.1\n",
      "GCF_000021405.1\n",
      "GCF_000012125.1\n",
      "GCF_000158275.2\n",
      "GCF_000016245.1\n",
      "GCF_000195715.1\n",
      "172594\n",
      "338520\n",
      "Number of reads: 338520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k value: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-cf4bfb1fe931>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     # Get kmers from removes all reads with unusual characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#     # Get kmers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mreads_list_kmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_kmers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kmers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreads_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m# #     print(unique_kmers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#     # Print out total number of kmers in dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-818c4ddafa2b>\u001b[0m in \u001b[0;36mget_kmers\u001b[0;34m(k_value, dict_seq)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mkmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mreads_list_kmer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkmer\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_kmers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0munique_kmers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
