import tensorflow as tf
from tensorflow.python.util import deprecation
deprecation._PRINT_DEPRECATION_WARNINGS = False
import numpy as np
from datetime import datetime

# Set model parameters
embedding_size = 6
num_layers = 1
lstm_size = 256
learning_rate = 0.0001
num_epochs = 5
batch_size = 64
k_value = 10
num_kmers = 4 ** k_value
num_classes = 8
sequence_length = int(150 - k_value + 1)

# Define the placeholders for holding the input values: sequences of unique kmers (tf_x)
# and the response values (tf_y)
def placeholders():
    tf_x = tf.compat.v1.placeholder(tf.int32, [batch_size, sequence_length], 'tf_x')
    tf_y = tf.compat.v1.placeholder(tf.int32, [batch_size], 'tf_y')
    tf_keepprob = tf.compat.v1.placeholder(tf.float32, name='tf_keepprob')
    return tf_x, tf_y, tf_keepprob

def createGraph(numThreads):
    # Create new empty computation graph
    g = tf.Graph()

    # Define model - LOADING MODEL AND RESTORING PARAMETERS ------------------
    with g.as_default():
        # Set a graph-level seed to make the random sequences generated by all operations be repeatable across sessions
        tf.random.set_seed(123)
        with tf.device('/device:GPU:0'):
            # Define the placeholders for holding the input values: sequences of unique kmers (tf_x)
            # and the response values (tf_y)
            tf_x, tf_y, tf_keepprob = placeholders()

            # Convert labels to a one-hot representation (necessary to use the cost function)
            y_onehot = tf.one_hot(indices=tf_y, depth=num_classes)

            # Create embedding layer: Create a matrix of size [n_kmers x embedding_size] as a tensor variable and
            # initialize its elements randomly with floats between [-1,1]
            embedding = tf.Variable(
                tf.random.uniform(shape=(num_kmers, embedding_size), minval=-1, maxval=1, name='embedding'))
            embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')

            # define LSTM cell and stack them together
            # BasicLSTMCell wrapper class that defines LSTM cells which can be stacked together to form a multilayer RNN
            # using the MultiRNNCell wrapper class, apply dropout (NN regularization) to each layer
            # make a list using python list comprehension of LSTM cells according to the desired number of RNN layers
            # cells = tf.keras.layers.StackedRNNCells([tf.compat.v1.nn.rnn_cell.DropoutWrapper( \
            #    tf.keras.layers.LSTMCell(lstm_size), output_keep_prob=tf_keepprob) for _ in range(num_layers)])

            cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([tf.compat.v1.nn.rnn_cell.DropoutWrapper(
                tf.compat.v1.nn.rnn_cell.LSTMCell(lstm_size, state_is_tuple=True),
                output_keep_prob=tf_keepprob) for i in range(num_layers)])

            # Define the initial state (there are 3 types of inputs in LSTM cells: input data x (embed_x data tensor),
            # activations of hidden units
            # from the previous time step h and the cell state from the previous time step C)
            saved_state = tf.compat.v1.get_variable('saved_state', shape=[num_layers, 2, batch_size, lstm_size],
                                                    dtype=tf.float32)
            state_placeholder = tf.compat.v1.placeholder(tf.float32, [num_layers, 2, batch_size, lstm_size],
                                                         name='state_placeholder')
            l = tf.unstack(state_placeholder, axis=0)
            tuple_initial_state = tuple(
                [tf.compat.v1.nn.rnn_cell.LSTMStateTuple(l[idx][0], l[idx][1]) for idx in range(num_layers)])
            print(f'\n<< initial state >> {tuple_initial_state}')

            # the tf.nn.dynamic_rnn function pulls the embedded data, the RNN cells and their initial states and creates
            # a pipeline for them according to the architecture of LSTM cells
            # We store the final state to use  as the initial state of the next mini-batch of data
            lstm_outputs, final_state = tf.compat.v1.nn.dynamic_rnn(cell=cells, inputs=embed_x,
                                                                    initial_state=tuple_initial_state)
            assign_state = tf.compat.v1.assign(saved_state, final_state, name='assign_state')

            # the tf.nn.dynamic_rnn function returns a tuple containing the activations of the RNN cells: outputs and their
            # final state: state. The output is a 3 dimensional tensor with the following shape:
            # lstm_outputs shape: [batch_size, max_time, cells.output_size]
            print(f'\n<< lstm_output >> {lstm_outputs}')
            print(f'\n<< final state >> {final_state}')
            print(f'{lstm_outputs[:, -1]}')

            # Pass outputs to a connected layer to get logits
            logits = tf.compat.v1.layers.dense(inputs=lstm_outputs[:, -1], units=num_classes, activation=None,
                                               name='logits')
            print(f'\n<< logits >> {logits}')

            y_proba = tf.nn.softmax(logits, name='probabilities')
            predictions = {
                'labels': tf.argmax(logits, axis=1, name='labels'),
                'probabilities': y_proba
            }
            print(f'\n<< predictions >> {predictions}')

            # Define the cost function
            cost = tf.reduce_mean(
                tf.nn.softmax_cross_entropy_with_logits(
                    logits=logits, labels=y_onehot),
                name='cost')
            print(f'\n<< cost >> {cost}')

            # Define the optimizer
            optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
            train_op = optimizer.minimize(cost, name='train_op')
        # Create saver object
        saver = tf.compat.v1.train.Saver()
        # Returns an Operation that initializes global variables in the graph
        init_op = tf.compat.v1.global_variables_initializer()

    # Test model with testing set
    print(f'\nStart testing: {datetime.now()}')
    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=numThreads,
                            inter_op_parallelism_threads=numThreads)

    with tf.compat.v1.Session(graph=g, config=config) as sess:
        latest_ckp = 'SpeciesRNN-sw-8orders-10-77.ckpt.meta'
        saver = tf.compat.v1.train.import_meta_graph('SpeciesRNN-sw-8orders-10-77.ckpt.meta')
        # restore the saved variable
        saver.restore(sess, latest_ckp)
        print('Model restored')

        '''# Check what the embeddings look like for the ‘AAAAAAAAAA’ 10-mer. --------------
        # Each 10-mer is represented first by an integer and then by an embedding vector before going through the neural network.
        # Get embeddings
        embeddingsValues = sess.run('embedding:0')
        print(embeddingsValues[0])
        for kmerString, kmerValue in kmers_dict.items():
            if kmerValue == 0:
                print('embedding of k-mer {0} with integer value {1} is {2}'.format(kmerString, kmerValue,
                                                                                    embeddingsValues[kmerValue]))
                break

        # START THE CLASSIFICATION -------------
        # START OF MULTIPROCESSING ---------------------

        preds = []  # keep test accuracy after each epoch for testing set
        probs = []
        test_state = np.zeros((num_layers, 2, batch_size, lstm_size))
        batch_generator_validation = create_batch_generator(X_test, None, batch_size=batch_size)
        for batch_x_test in batch_generator_validation:
            feed_val = {'tf_x:0': batch_x_test, 'tf_keepprob:0': 1.0, 'state_placeholder:0': test_state}
            pred, test_state, prob = sess.run(['labels:0', saved_state, 'probabilities:0'], feed_dict=feed_val)
            preds.append(pred)
            probs.append(prob)
        all_preds = np.concatenate(preds)
        all_probs = np.concatenate(probs)
        print(f'Size all_probs vector: {len(all_probs)}')
        print(f'Size all_preds vector: {len(all_preds)}')
        # Get performance statistics for each order
        model_performance(all_preds, all_probs, ListReadsID, FastqFile, Reads_dict, class_mapping)
    print(f'End testing: {datetime.now()}\n')'''


# Function to generate batches of data
def create_batch_generator(x, y=None, batch_size=64):
    # get the number of batches of size 64 given the size of the training dataset
    n_batches = len(x) // batch_size  # 2 / will round the float number to an integer
    # create empty array to store y values in case it is not none
    y_copy = np.empty([1, n_batches * batch_size])
    if y is not None:
        y_copy = np.array(y)
        y_copy = y_copy[:n_batches * batch_size]
    # create batch from reduced training set
    x_copy = np.array(x)
    x_copy = x_copy[:n_batches * batch_size]
    for i in range(0, x_copy.shape[0], batch_size):
        if y is not None:
            yield (x_copy[i:i + batch_size, :], y_copy[i:i + batch_size])
        else:
            yield x_copy[i:i + batch_size, :]


def model_performance(preds, probs, ListReadsID, FastqFile, Reads_dict, class_mapping):
    # Create dictionary with number of reads assigned to each class
    DictProbs = {}  # key = order, value = list of probabilities for each read predicted
    ListProbsReadsDropped = []

    for i in range(len(preds)):
        prediction = probs[i][preds[i]]
        # Get only prediction with probability above 50%
        if prediction <= 0.5:
            ListProbsReadsDropped.append(probs[i])
            continue

        # Get Order predicted
        predOrder = class_mapping[preds[i]]
        # Add read to corresponding file
        with gzip.open('{0}-Reads-{1}.fq.gz'.format(FastqFile.split(".")[0], OrderName),
                       'at') as ReadsClassified:
            # Create an instance with fastq reads information
            if ListReadsID[i] in Reads_dict:
                seq_record = Reads_dict[ListReadsID[i]]
                ReadsClassified.write(seq_record.format('fastq'))
            # add read to DictProbs
            if predOrder not in DictProbs:
                DictProbs[predOrder] = []
            DictProbs[predOrder].append(prediction)

    ResultsFile = open('Predictions-Set24.txt', 'a+')
    ResultsFile.write(
        'Train set: ' + str(24) + '\t' + 'Checkpoint File: ' + str(77) + '\t' + 'Number of reads tested: ' + str(
            len(preds)) + '\n')
    for OrderName, ListProbs in DictProbs.items():
        ResultsFile.write(str(OrderName) + '\t' + 'Number of Reads: ' + str(len(ListProbs)) + '\t')
        for prob in ListProbs:
            ResultsFile.write(str(prob) + '\t')
        ResultsFile.write('\n')
