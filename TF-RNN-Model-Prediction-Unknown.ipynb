{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Implementation of an artificial neural network\n",
    "# Load all necessary libraries\n",
    "from __future__ import division, print_function\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from operator import add\n",
    "import datetime \n",
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "from Bio import SeqIO\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate batches of data\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    # get the number of batches of size 64 given the size of the training dataset\n",
    "    n_batches = len(x)//batch_size # 2 / will round the float number to an integer\n",
    "    # create empty array to store y values in case it is not none\n",
    "    y_copy = np.empty([1, n_batches*batch_size])\n",
    "    if y is not None:\n",
    "        y_copy = np.array(y)\n",
    "        y_copy = y_copy[:n_batches*batch_size]\n",
    "    # create batch from reduced training set \n",
    "    x_copy = np.array(x)\n",
    "    x_copy = x_copy[:n_batches*batch_size]\n",
    "    for i in range(0, x_copy.shape[0], batch_size):\n",
    "        if y is not None:\n",
    "            yield (x_copy[i:i+batch_size, :], y_copy[i:i+batch_size])\n",
    "        else:\n",
    "            yield x_copy[i:i+batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(preds, probs, ListReadsID, FastqFile, class_mapping, Filename):\n",
    "    # Create dictionary with number of reads assigned to each class\n",
    "    DictProbs = {} # key = order, value = list of probabilities for each read predicted\n",
    "    ReadsDropped = 0\n",
    "    ListProbsReadsDropped = []\n",
    "    fastq_in = gzip.open(FastqFile, 'rt')\n",
    "    # Create an instance with fastq reads information\n",
    "    Reads = SeqIO.parse(fastq_in,'fastq')\n",
    "    for i in range(len(preds)):\n",
    "        # Get only prediction with probability above 50%\n",
    "        if probs[i][preds[i]] > 0.5:\n",
    "#             print('probabilities: {}'.format(probs[i]))\n",
    "#             print('probability for prediction: {}'.format(probs[i][preds[i]]))\n",
    "#             print('prediction: {}'.format(preds[i]))\n",
    "            # Get Order predicted\n",
    "            predOrder = str()\n",
    "            for OrderName, OrderValue in class_mapping.items():\n",
    "                if OrderValue == preds[i]:\n",
    "                    predOrder = OrderName\n",
    "            # Add read to corresponding file\n",
    "            with open('/glade/u/home/ccres/data/{0}-Reads-{1}.fq.gz'.format(Filename,OrderName), 'at') as ReadsClassified:\n",
    "                for record in Reads:\n",
    "                    if record.id == ListReadsID[i]:\n",
    "                        ReadsClassified.write(record.format('fastq'))\n",
    "            # add read to DictProbs\n",
    "            if predOrder not in DictProbs:\n",
    "                DictProbs[predOrder] = [probs[i][preds[i]]]\n",
    "            else:\n",
    "                DictProbs[predOrder].append(probs[i][preds[i]])\n",
    "        else:\n",
    "            ReadsDropped += 1\n",
    "            ListProbsReadsDropped.append(probs[i])\n",
    "    ResultsFile = open('/glade/u/home/ccres/data/Predictions-Set{}.txt'.format(24),'a+')\n",
    "    ResultsFile.write('Train set: ' + str(24) + '\\t' + 'Checkpoint File: ' +  str(77) + '\\t' + 'Number of reads tested: ' + str(len(preds)) + '\\n')\n",
    "    for OrderName, ListProbs in DictProbs.items():\n",
    "        ResultsFile.write(str(OrderName) + '\\t' + 'Number of Reads: ' + str(len(ListProbs)) + '\\t')\n",
    "        for prob in ListProbs:\n",
    "            ResultsFile.write(str(prob) + '\\t')\n",
    "        ResultsFile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_kmers(list_nt, prefix, n, k, list_kmers):\n",
    "    if k == 0 :\n",
    "        list_kmers.append(prefix)\n",
    "        return list_kmers\n",
    "    \n",
    "    for i in range(n):\n",
    "        newPrefix = prefix + list_nt[i]\n",
    "        get_all_kmers(list_nt, newPrefix, n, k-1, list_kmers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that looks for any characters different \n",
    "# from A, C, T or G and converts the DNA sequence into a vector of 10-mers \n",
    "def ParseSeq(DNAsequence,kmers_dict):\n",
    "    # create empty list to store all the kmers\n",
    "    listKmers = []\n",
    "    # create empty string to store each kmer\n",
    "    kmer = str()\n",
    "    for n in range(len(DNAsequence)):\n",
    "        # Iterate through DNA sequence\n",
    "        if n < len(DNAsequence) - (10 - 1):\n",
    "            if DNAsequence[n] not in ['A', 'T', 'C', 'G']:\n",
    "                return listKmers\n",
    "            else:\n",
    "                # add character to kmer string\n",
    "                if len(kmer) < 10:\n",
    "                    kmer += DNAsequence[n]\n",
    "                else:\n",
    "                    # lookup integer mapped to the kmer\n",
    "                    kmer_Integer = kmers_dict[kmer]\n",
    "                    # Add kmer to vector of kmer\n",
    "                    listKmers.append(kmer_Integer)\n",
    "                    # reinitialize string\n",
    "                    kmer = str()\n",
    "    # \n",
    "    if len(listKmers) < 141:\n",
    "        NumberZerosToAdd = 141 - len(listKmers)\n",
    "        for i in range(NumberZerosToAdd):\n",
    "            listKmers.append(0)\n",
    "    else:\n",
    "        # Get the first 141 kmers\n",
    "        listKmers = listKmers[:141]\n",
    "    # transform list into numpy array\n",
    "    array_int = np.asarray(listKmers)\n",
    "    # Flip array\n",
    "    array_int = np.flip(array_int, axis=0)\n",
    "    if len(array_int) != 141:\n",
    "        print('Size of kmer array: {}'.format(len(array_int)))\n",
    "       \tprint('DNA sequence: {}'.format(DNAsequence))\n",
    "    return array_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetKmersDictionary(k_value=10):\n",
    "    # Create empty list to store all possible kmers\n",
    "    list_kmers = []\n",
    "    list_nt = ['A', 'T', 'C', 'G']\n",
    "    # Get list of all possible 4**k_value kmers\n",
    "    get_all_kmers(list_nt, \"\", len(list_nt), k_value, list_kmers)\n",
    "    # generate a list of integers\n",
    "    list_num = list(range(0,len(list_kmers)))\n",
    "    # Assign an integer to each kmer\n",
    "    kmers_dict = dict(zip(list_kmers, list_num))\n",
    "    return kmers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseFastq(FileFastq):\n",
    "    DictVectors = {} # keys = read_id, value = array of integer\n",
    "    kmers_dict = GetKmersDictionary()\n",
    "    total_number_reads = 0\n",
    "    fastq_in = gzip.open(FileFastq, 'rt')\n",
    "    # Create an instance with fastq reads information\n",
    "    Reads = SeqIO.parse(fastq_in,'fastq')\n",
    "    record_dict = SeqIO.to_dict(Reads)\n",
    "    # iterate through fastq file\n",
    "    for record in Reads:\n",
    "        total_number_reads += 1\n",
    "        # Check read sequence\n",
    "        KmerVector = ParseSeq(record.seq, kmers_dict)\n",
    "        if len(KmerVector) == 141:\n",
    "            DictVectors[record.id] = KmerVector\n",
    "    print('Total number of reads: {}'.format(total_number_reads))\n",
    "    print('Number of reads after processing: {}'.format(len(DictVectors)))\n",
    "    return DictVectors, Reads, kmers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set model parameters\n",
    "    embedding_size = 6\n",
    "    num_layers = 1\n",
    "    lstm_size = 256\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 5\n",
    "    batch_size = 64\n",
    "    k_value = 10\n",
    "    num_kmers = 4**k_value\n",
    "    num_classes = 8\n",
    "    sequence_length = int(150 - k_value + 1)\n",
    "    \n",
    "    print(\"\\nProcess reads: START -- {}\".format(datetime.datetime.now()),file=sys.stderr)\n",
    "    # Get fastq file name\n",
    "    FastqFile = str(sys.argv[1])\n",
    "    print('Fastq File: {}'.format(FastqFile))\n",
    "    # Get filename \n",
    "    FileName = str(sys.argv[1]).split('.')[0]\n",
    "    print(FileName)\n",
    "    # Parse Fastq file\n",
    "    DictVectors, Reads, kmers_dict = ParseFastq(FastqFile)\n",
    "    # Create a list with the reads id\n",
    "    ListReadsID = list(DictVectors.keys())\n",
    "    # Create matrix of zeros, where each row corresponds to a read (vector of kmers)\n",
    "    sequences = np.zeros((len(DictVectors), sequence_length), dtype=int)  \n",
    "    # Replace array in corresponding position in sequences matrix \n",
    "    # (the index of the read in ListReadsID corresponds to the position of it's kmer vector \n",
    "    # in the sequences matrix)\n",
    "    for i in range(len(ListReadsID)):\n",
    "        sequences[i] = DictVectors[ListReadsID[i]]\n",
    "    X_test = sequences  \n",
    "    print(\"\\nProcess reads: END -- {}\".format(datetime.datetime.now()), file=sys.stderr)\n",
    "\n",
    "    # Get order names mapped to integers\n",
    "    class_mapping = {'Enterobacterales': 0, 'Mycoplasmatales': 1, 'Chlamydiales': 2, 'Vibrionales': 3,\n",
    "                     'Fusobacteriales': 4, 'Spirochaetales': 5, 'Rhodobacterales': 6, 'Unclassified': 7}\n",
    "    print('class_mapping dictionay: {}'.format(class_mapping),file=sys.stderr)\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Create new empty computation graph\n",
    "    g = tf.Graph()\n",
    "    # Define model\n",
    "    with g.as_default():\n",
    "        # Set a graph-level seed to make the random sequences generated by all operations be repeatable across sessions\n",
    "        tf.set_random_seed(123)\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            # Define the placeholders for holding the input values: sequences of unique kmers (tf_x) \n",
    "            # and the response values (tf_y)\n",
    "            tf_x = tf.placeholder(dtype=tf.int32, shape=[batch_size, sequence_length], name='tf_x')\n",
    "            print(tf_x, file=sys.stderr)\n",
    "            tf_y = tf.placeholder(dtype=tf.int32, shape=[batch_size], name='tf_y')\n",
    "            print(tf_y, file=sys.stderr)\n",
    "            tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
    "            print(tf_keepprob, file=sys.stderr)\n",
    "            # Convert labels to a one-hot representation (necessary to use the cost function)\n",
    "            y_onehot = tf.one_hot(indices=tf_y, depth=num_classes)\n",
    "            print(y_onehot, file=sys.stderr)\n",
    "            # Create embedding layer: Create a matrix of size [n_kmers x embedding_size] as a tensor variable and initialize its elements\n",
    "            # randomly with floats between [-1,1]\n",
    "            embedding = tf.Variable(tf.random_uniform((num_kmers, embedding_size), minval=-1, maxval=1), name='embedding')\n",
    "            print(embedding, file=sys.stderr)\n",
    "            embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
    "            print(embed_x, file=sys.stderr)\n",
    "            # define LSTM cell and stack them together\n",
    "            # BasicLSTMCell wrapper class that defines LSTM cells which can be stacked together to form a multilayer RNN\n",
    "            # using the MultiRNNCell wrapper class, apply dropout (NN regularization) to each layer\n",
    "            # make a list using python list comprehension of LSTM cells according to the desired number of RNN layers\n",
    "    #             cells = tf.contrib.rnn.MultiRNNCell(\n",
    "    #                 [tf.contrib.rnn.DropoutWrapper(\n",
    "    #                     tf.nn.rnn_cell.LSTMCell(lstm_size), \n",
    "    #                     output_keep_prob=tf_keepprob)\n",
    "    #                     for i in range(num_layers)])\n",
    "            cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "                    tf.nn.rnn_cell.LSTMCell(lstm_size,state_is_tuple=True), \n",
    "                    output_keep_prob=tf_keepprob)for i in range(num_layers)])\n",
    "            print(cells)\n",
    "    #         tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell')\n",
    "            # Define the initial state (there are 3 types of inputs in LSTM cells: input data x (embed_x data tensor), \n",
    "            # activations of hidden units\n",
    "            # from the previous time step h and the cell state from the previous time step C)\n",
    "            # when we start processing a new input sequence, we initialize the cell states to zero state\n",
    "    #             initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    #             saved_c = tf.get_variable('saved_c', shape=[batch_size,lstm_size], dtype=tf.float32)\n",
    "    #             saved_h = tf.get_variable('saved_h', shape=[batch_size,lstm_size], dtype=tf.float32)\n",
    "            saved_state = tf.get_variable('saved_state', shape=[num_layers, 2, batch_size, lstm_size], dtype=tf.float32)\n",
    "    #             initial_c = tf.placeholder(dtype=tf.float32, shape=[batch_size,lstm_size], name='initial_c')\n",
    "    #             initial_h = tf.placeholder(dtype=tf.float32, shape=[batch_size,lstm_size], name='initial_h')\n",
    "            state_placeholder = tf.placeholder(tf.float32, [num_layers, 2, batch_size, lstm_size], name='state_placeholder')\n",
    "            l = tf.unstack(state_placeholder, axis=0)\n",
    "    #             initial_state = tf.nn.rnn_cell.LSTMStateTuple(c=initial_c, h=initial_h)\n",
    "            tuple_initial_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(l[idx][0],l[idx][1])for idx in range(num_layers)])\n",
    "            print(' << initial state >>', tuple_initial_state, file=sys.stderr)\n",
    "    #         with tf.device('/device:GPU:1'):\n",
    "            # the tf.nn.dynamic_rnn function pulls the embedded data, the RNN cells and their initial states and creates\n",
    "            # a pipeline for them according to the architecture of LSTM cells\n",
    "            # We store the final state to use  as the initial state of the next mini-batch of data\n",
    "            lstm_outputs, final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=tuple_initial_state)\n",
    "    #             assign_c = tf.assign(saved_c, final_state.c)\n",
    "    #             assign_h = tf.assign(saved_h, final_state.h)\n",
    "            assign_state = tf.assign(saved_state, final_state, name='assign_state')\n",
    "    #             with tf.control_dependencies([assign_state]):\n",
    "    #                 assign_op = tf.no_op(name='assign_op')\n",
    "    #             with tf.control_dependencies([assign_state]):\n",
    "    #                 assign_op = tf.no_op()\n",
    "            # the tf.nn.dynamic_rnn function returns a tuple containing the activations of the RNN cells: outputs and their\n",
    "            # final state: state. The output is a 3 dimensional tensor with the following shape:\n",
    "            # lstm_outputs shape: [batch_size, max_time, cells.output_size]\n",
    "            print('\\n << lstm_output >>', lstm_outputs, file=sys.stderr)\n",
    "            print('\\n << final state >>', final_state, file=sys.stderr)\n",
    "            print(lstm_outputs[:, -1], file=sys.stderr)\n",
    "            # Pass outputs to a connected layer to get logits \n",
    "            logits = tf.layers.dense(inputs=lstm_outputs[:, -1], units=num_classes, activation=None, name='logits')\n",
    "            print('\\n << logits >>', logits)\n",
    "            y_proba = tf.nn.softmax(logits, name='probabilities')\n",
    "            predictions = {\n",
    "                'labels': tf.argmax(logits, axis=1, name='labels'),\n",
    "                'probabilities': y_proba\n",
    "            }\n",
    "            print('\\n << predictions >>', predictions, file=sys.stderr)\n",
    "            # Define the cost function\n",
    "            cost = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y_onehot),\n",
    "                name='cost')\n",
    "            print('\\n << cost >>', cost, file=sys.stderr)\n",
    "            # Define the optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(cost, name='train_op')\n",
    "        # Create saver object\n",
    "        saver = tf.train.Saver()\n",
    "        # Returns an Operation that initializes global variables in the graph\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # Test model with testing set\n",
    "    print(\"\\nStart testing: {}\".format(datetime.datetime.now()), file=sys.stderr)\n",
    "    with tf.Session(graph = g) as sess:\n",
    "        latest_ckp = '/glade/u/home/ccres/run/Model-balanced-150bp/model-8orders-sw-k10-balanced/SpeciesRNN-sw-8orders-10-77.ckpt'\n",
    "        print(latest_ckp, file=sys.stderr)\n",
    "        saver = tf.train.import_meta_graph('/glade/u/home/ccres/run/Model-balanced-150bp/model-8orders-sw-k10-balanced/SpeciesRNN-sw-8orders-10-77.ckpt.meta')\n",
    "        # restore the saved variable\n",
    "        saver.restore(sess, latest_ckp)\n",
    "        print('Model restored')\n",
    "        # Get embeddings\n",
    "        embeddingsValues = sess.run('embedding:0')\n",
    "        print(embeddingsValues[0])\n",
    "        for kmerString, kmerValue in kmers_dict.items():\n",
    "            if kmerValue == 0:\n",
    "                print('embedding of k-mer {0} with integer value {1} is {2}'.format(kmerString, kmerValue, embeddingsValues[kmerValue]))\n",
    "                break\n",
    "        \n",
    "        preds = []  # keep test accuracy after each epoch for testing set\n",
    "        probs = []\n",
    "        test_state = np.zeros((num_layers, 2, batch_size, lstm_size))\n",
    "        batch_generator_validation = create_batch_generator(X_test, None, batch_size=batch_size)\n",
    "        for batch_x_test in batch_generator_validation:\n",
    "            feed_val = {'tf_x:0':batch_x_test, 'tf_keepprob:0': 1.0, 'state_placeholder:0':test_state}\n",
    "            pred, test_state, prob = sess.run(['labels:0', saved_state, 'probabilities:0'], feed_dict=feed_val)\n",
    "            preds.append(pred)\n",
    "            probs.append(prob)\n",
    "        all_preds = np.concatenate(preds)\n",
    "        all_probs = np.concatenate(probs)\n",
    "        print('Size all_probs vector: {}'.format(len(all_probs)))\n",
    "        print('Size all_preds vector: {}'.format(len(all_preds)))\n",
    "        # Get performance statistics for each order\n",
    "        model_performance(all_preds, all_probs, ListReadsID, FastqFile, class_mapping, FileName)\n",
    "    print(\"\\nEnd testing: {}\".format(datetime.datetime.now()), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Process reads: START -- 2019-12-30 09:57:51.144416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fastq File: /Users/Cissou/Desktop/mapped_ZTP1_S25_L006_R1_unpaired_trimmed.fq.gz\n",
      "mapped_ZTP1_S25_L006_R1_unpaired_trimmed\n",
      "Total number of reads: 0\n",
      "Number of reads after processing: 0\n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0xc33b26b00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Process reads: END -- 2019-12-30 09:59:00.420227\n",
      "class_mapping dictionay: {'Enterobacterales': 0, 'Mycoplasmatales': 1, 'Chlamydiales': 2, 'Vibrionales': 3, 'Fusobacteriales': 4, 'Spirochaetales': 5, 'Rhodobacterales': 6, 'Unclassified': 7}\n",
      "Tensor(\"tf_x:0\", shape=(64, 141), dtype=int32)\n",
      "Tensor(\"tf_y:0\", shape=(64,), dtype=int32)\n",
      "Tensor(\"tf_keepprob:0\", dtype=float32)\n",
      "Tensor(\"one_hot:0\", shape=(64, 8), dtype=float32)\n",
      "<tf.Variable 'embedding:0' shape=(1048576, 6) dtype=float32_ref>\n",
      "Tensor(\"embeded_x/Identity:0\", shape=(64, 141, 6), dtype=float32)\n",
      " << initial state >> (LSTMStateTuple(c=<tf.Tensor 'strided_slice:0' shape=(64, 256) dtype=float32>, h=<tf.Tensor 'strided_slice_1:0' shape=(64, 256) dtype=float32>),)\n",
      "\n",
      " << lstm_output >> Tensor(\"rnn/transpose_1:0\", shape=(64, 141, 256), dtype=float32)\n",
      "\n",
      " << final state >> (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(64, 256) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(64, 256) dtype=float32>),)\n",
      "Tensor(\"strided_slice_2:0\", shape=(64, 256), dtype=float32)\n",
      "\n",
      " << predictions >> {'labels': <tf.Tensor 'labels:0' shape=(64,) dtype=int64>, 'probabilities': <tf.Tensor 'probabilities:0' shape=(64, 8) dtype=float32>}\n",
      "\n",
      " << cost >> Tensor(\"cost:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " << logits >> Tensor(\"logits/BiasAdd:0\", shape=(64, 8), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-8068029a6a21>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_op'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Create saver object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;31m# Returns an Operation that initializes global variables in the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/env-DL/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1100\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m   1101\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/env-DL/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/env-DL/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1137\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
